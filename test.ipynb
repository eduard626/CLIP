{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c947a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_path = \"/home/eduardo/Downloads/rgbd_dataset_freiburg1_desk2/rgb/\"\n",
    "\n",
    "images = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "print(\"Found images:\", len(images))\n",
    "\n",
    "sample_image_path = images[0]\n",
    "print(\"Image path:\", sample_image_path)\n",
    "sample_image = Image.open(sample_image_path).convert(\"RGB\")\n",
    "print(\"Sample image shape:\", sample_image.size)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)\n",
    "image = preprocess(sample_image).unsqueeze(0).to(device)\n",
    "print(\"Image shape after preprocessing:\", image.shape)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    print(\"Image features shape:\", image_features.shape)\n",
    "    # forward features\n",
    "    feats = model.encode_image(image, dense_features=True)\n",
    "    print(\"Dense features shape:\", feats.shape)\n",
    "    feats = feats[:, 1:, :] # remove the CLS token\n",
    "\n",
    "# VIT-L/14@336px used 336px input size, with 14 patches per side\n",
    "# so the output feature map will be 336/14 = 24 patches per side\n",
    "# and each patch is 336/14 = 24 pixels wide\n",
    "patch_size = 336 // 14\n",
    "print(\"Patch size:\", patch_size)\n",
    "patch_embeddings = feats.reshape(1, 24, 24, -1).permute(0, 3, 1, 2) # (N, C, H, W)\n",
    "print(\"Patch embeddings shape:\", patch_embeddings.shape)\n",
    "# Upsample the patch embeddings to the original image size\n",
    "upsampled_embeddings = torch.nn.functional.interpolate(\n",
    "    patch_embeddings,\n",
    "    size=(image.shape[2], image.shape[3]),\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ")\n",
    "print(\"Upsampled embeddings shape:\", upsampled_embeddings.shape)\n",
    "\n",
    "# Lets see the 3 PC components of the upsampled embeddings\n",
    "# We will use PCA to reduce the dimensionality to 3\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "upsampled_embeddings_np = upsampled_embeddings.squeeze(0).cpu().numpy()\n",
    "print(\"Upsampled embeddings shape (numpy):\", upsampled_embeddings_np.shape) # (768, 336, 336)\n",
    "pca = PCA(n_components=3)\n",
    "pca_embeddings = pca.fit_transform(upsampled_embeddings_np.reshape(768, -1).T).T\n",
    "print(\"PCA embeddings shape:\", pca_embeddings.shape) # (3, 336, 336)\n",
    "pca_embeddings = pca_embeddings.reshape(3, 336, 336)\n",
    "print(\"PCA embeddings shape after reshape:\", pca_embeddings.shape) # (3, 336, 336)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a06261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets display the PCA embeddings as RGB channels\n",
    "import matplotlib.pyplot as plt\n",
    "_, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "input_image = image.squeeze(0).cpu().permute(1, 2, 0).numpy()  # (336, 336, 3)\n",
    "# Normalize the input image to [0, 1]\n",
    "input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "input_image = input_image * 255  # Scale to [0, 255]\n",
    "input_image = input_image.astype(np.uint8)  # Convert to uint8\n",
    "ax[0].imshow(input_image)\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].axis('off')\n",
    "pca_image = (pca_embeddings - pca_embeddings.min()) / (pca_embeddings.max() - pca_embeddings.min()\n",
    ")  # Normalize to [0, 1]\n",
    "pca_image = pca_image.transpose(1, 2, 0)  # (336, 336, 3)\n",
    "ax[1].imshow(pca_image)\n",
    "ax[1].set_title(\"PCA Embeddings\")\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test torch conv2d params\n",
    "input_dim = 3 # RGB image\n",
    "output_dim = 128 # Example output dimension\n",
    "kernel_size = 3 # Example kernel size - default for many conv layers\n",
    "stride = 1 # Default stride\n",
    "padding = 1 # Default padding for 'same' convolution\n",
    "conv_layer = torch.nn.Conv2d(input_dim, output_dim, kernel_size, stride, padding).to(device)\n",
    "max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0).to(device)\n",
    "print(\"Conv2d layer:\", conv_layer)\n",
    "\n",
    "#dim maths\n",
    "# Input: (N, C_in, H_in, W_in) -> Output: (N, C_out, H_out, W_out)\n",
    "# Where:\n",
    "# N = batch size\n",
    "# C_in = input channels (e.g., 3 for RGB)\n",
    "# C_out = output channels (e.g., 256)\n",
    "# H_in = input height\n",
    "# W_in = input width\n",
    "# H_out = (H_in + 2*padding - kernel_size) // stride + 1\n",
    "h_out = (image.shape[2] + 2 * padding - kernel_size) // stride + 1\n",
    "w_out = (image.shape[3] + 2 * padding - kernel_size) // stride + 1\n",
    "print(\"Output dimensions: H_out =\", h_out, \", W_out =\", w_out)\n",
    "# W_out = (W_in + 2*padding - kernel_size) // stride + 1\n",
    "feature_map = conv_layer(image)\n",
    "reduced = max_pool(feature_map)\n",
    "print(\"Feature map shape after conv2d:\", feature_map.shape)\n",
    "print(\"Reduced feature map shape after max pooling:\", reduced.shape)\n",
    "# how many parameters in conv layer?\n",
    "# each filter has (input_dim * kernel_size * kernel_size) weights + 1 bias\n",
    "# we have output_dim filters, so\n",
    "num_params = (input_dim * kernel_size * kernel_size + 1) * output_dim\n",
    "print(\"Number of parameters in conv layer:\", num_params)\n",
    "print(\"Total parameters in conv layer:\", sum(p.numel() for p in conv_layer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d268eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
